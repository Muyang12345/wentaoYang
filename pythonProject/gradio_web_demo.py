# Copyright (c) Alibaba Cloud.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""A simple web interactive chat demo based on gradio."""
import os
from argparse import ArgumentParser

import gradio as gr
import mdtex2html

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

DEFAULT_CKPT_PATH = 'Qwen-14B-Chat'


def _get_args():
    parser = ArgumentParser()
    parser.add_argument("-c", "--checkpoint-path", type=str, default=DEFAULT_CKPT_PATH,
                        help="Checkpoint name or path, default to %(default)r")
    parser.add_argument("--cpu-only", action="store_true", help="Run demo with CPU only")

    parser.add_argument("--share", action="store_true", default=False,
                        help="Create a publicly shareable link for the interface.")
    parser.add_argument("--inbrowser", action="store_true", default=False,
                        help="Automatically launch the interface in a new tab on the default browser.")
    parser.add_argument("--server-port", type=int, default=8000,
                        help="Demo server port.")
    parser.add_argument("--server-name", type=str, default="127.0.0.1",
                        help="Demo server name.")

    args = parser.parse_args()
    return args


def _load_model_tokenizer(args):
    tokenizer = AutoTokenizer.from_pretrained(
        args.checkpoint_path, trust_remote_code=True, resume_download=True,
    )

    if args.cpu_only:
        device_map = "cpu"
    else:
        device_map = "auto"

    model = AutoModelForCausalLM.from_pretrained(
        args.checkpoint_path,
        device_map=device_map,
        trust_remote_code=True,
        resume_download=True,
    ).eval()

    config = GenerationConfig.from_pretrained(
        args.checkpoint_path, trust_remote_code=True, resume_download=True,
    )

    return model, tokenizer, config


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert(message),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def _parse_text(text):
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = f"<br></code></pre>"
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace("`", r"\`")
                    line = line.replace("<", "&lt;")
                    line = line.replace(">", "&gt;")
                    line = line.replace(" ", "&nbsp;")
                    line = line.replace("*", "&ast;")
                    line = line.replace("_", "&lowbar;")
                    line = line.replace("-", "&#45;")
                    line = line.replace(".", "&#46;")
                    line = line.replace("!", "&#33;")
                    line = line.replace("(", "&#40;")
                    line = line.replace(")", "&#41;")
                    line = line.replace("$", "&#36;")
                lines[i] = "<br>" + line
    text = "".join(lines)
    return text


def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


class UserSession:
    def __init__(self):
        self.task_history = []  # å­˜å‚¨ç”¨æˆ·çš„ä»»åŠ¡å†å²
        self.awaiting_details = False  # æ ‡è®°æ˜¯å¦ç­‰å¾…ç”¨æˆ·æä¾›æ›´å¤šç»†èŠ‚
        self.last_question_type = None  # ä¸Šä¸€æ¬¡é—®é¢˜çš„ç±»å‹
        self.height = 0
        self.weight = 0

    def add_question(self, question_type, details=None):
        self.task_history.append((question_type, details))
        self.last_question_type = question_type
        self.awaiting_details = question_type in ["size_inquiry", "suitability_inquiry"] and not details

    def update_last_question(self, details):
        if self.task_history:
            last_question = self.task_history[-1]
            self.task_history[-1] = (last_question[0], details)
            self.awaiting_details = False

    def is_awaiting_details(self):
        return self.awaiting_details

    def get_last_question(self):
        return self.task_history[-1] if self.task_history else (None, None)


def _launch_demo(args, model, tokenizer, config):
    import re
    session = UserSession()
    products = '''
        1	777711402834	ä¸é€€æ¢ ç¾å›½ç›´é‚® DUCHAMP åŒè‚©åŒ…ã€16637ã€‘	https://item.taobao.com/item.htm?id=777711402834		
				5314268376342	ç°è‰²ã€882ã€‘;ç›´é‚®
2	771267501017	ç¾å›½ç›´é‚® ä¸æ”¯æŒé€€æ¢ã€æ‹ä¸‹é€€æ¬¾çš„å®šé‡‘ä¸é€€ã€‘	https://item.taobao.com/item.htm?id=771267501017		
				5317693377411	é»‘è‰²
				5317693377410	ç™½è‰²
3	768106449049	ä¸é€€æ¢ ç¾å›½ç›´é‚®DKNY  æ–œæŒåŒ…ã€R33AAA60ã€‘	https://item.taobao.com/item.htm?id=768106449049		
				5273111029099	é»‘è‰²;ç›´é‚®
4	759582177394	ä¸é€€æ¢ç¾å›½ç›´é‚® ACT*IVE  æ¸…æ´å‰‚æ³¡*è…¾*ç‰‡ 24ç‰‡	https://item.taobao.com/item.htm?id=759582177394		
				5405658246136	æ´—è¡£æœºæ³¡è…¾ç‰‡;ç›´é‚®
				5405658246137	æ´—ç¢—æœºæ¸…æ´—æ³¡è…¾ç‰‡;ç›´é‚®
5	750438652376	æ–° ä¸é€€æ¢ç¾å›½ç›´é‚® 32&deg;HEAT å¥³å£«æŠ½ç»³ä¼‘é—²è£¤ ä¸åŠ ç»’ å†…	https://item.taobao.com/item.htm?id=750438652376		
				5177499909869	é»‘è‰²ã€1409242ã€‘;XSè…°å›´70
				5177499909870	é»‘è‰²ã€1409242ã€‘;Sè…°å›´74
				5177499909871	é»‘è‰²ã€1409242ã€‘;Mè…°å›´78
				5177499909872	é»‘è‰²ã€1409242ã€‘;Lè…°å›´86
				5177499909873	é»‘è‰²ã€1409242ã€‘;XLè…°å›´92
				5403822967041	è“è‰²ã€1409242ã€‘;XSè…°å›´70
				5403822967039	è“è‰²ã€1409242ã€‘;Sè…°å›´74
				5403822967038	è“è‰²ã€1409242ã€‘;Mè…°å›´78
				5403822967037	è“è‰²ã€1409242ã€‘;Lè…°å›´86
				5403822967040	è“è‰²ã€1409242ã€‘;XLè…°å›´92
    '''
    cm_info = '''
    å¥³å£«å°ºå¯¸	XS	S	M	L	XL
èº«é«˜	155-160	158-168	160-168	160-170	160-170
ä½“é‡	85-95	110-120	120-130	130ä»¥ä¸Š	140ä»¥ä¸Š

    '''

    def parse_user_input(input_text):
        # è¯†åˆ«å°ºç è¯¢é—®
        size_match = re.search(r"size_inquiry", input_text)
        if size_match:
            details = size_match.groups()
            return "size_inquiry", {"details": details}

        # è¯†åˆ«å•†å“é€‚é…æ€§è¯¢é—®
        keyword_match = re.search(r"product_inquiry", input_text)
        if keyword_match:
            product_id = keyword_match.groups()
            return "product_inquiry", {"product_id": product_id}

        return "unknown", {}

    def predict(_query, _chatbot, _task_history):
        print(f"User: {_parse_text(_query)}")
        _chatbot.append((_parse_text(_query), ""))
        full_response = ""

        for response in model.chat_stream(tokenizer, _query, history=_task_history, generation_config=config):
            _chatbot[-1] = (_parse_text(_query), _parse_text(response))

            yield _chatbot
            full_response = _parse_text(response)

        print(f"History: {_task_history}")
        _task_history.append((_query, full_response))
        print(f"Qwen-Chat: {_parse_text(full_response)}")

    def predict1(_query, _chatbot, _task_history):
        # åˆ†æç”¨æˆ·è¾“å…¥ï¼Œå†³å®šæ˜¯å¦éœ€è¦è¡¥å……ä¿¡æ¯

        modified_query = _query
        if session.last_question_type is None:
            modified_query = (f"ç”¨æˆ·é—®é¢˜æ˜¯{_query}. æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œç¡®è®¤ç”¨æˆ·çš„é—®é¢˜ç±»å‹ï¼Œå¦‚æœæ˜¯èº«é«˜ä½“é‡é—®å°ºç åˆä¸åˆé€‚ï¼Œæˆ–è€…å°ºç "
                              f"åº”è¯¥å¤šå°‘èº«é«˜ä½“é‡ï¼Œè¯·åªè¿”å›{{size_inquiry,èº«é«˜,ä½“é‡,å°ºç }},æå–ä¸åˆ°çš„ä¿¡æ¯è¯·ç½®ä¸º0ï¼›"
                              f"å¦‚æœæ˜¯é—®æŸä¸ªå•†å“ä¿¡æ¯æˆ–è€…æŸä¸ªé“¾æ¥ä¿¡æ¯ï¼Œå¦‚å•†å“çš„é¢œè‰²ï¼Œåº“å­˜ï¼Œèƒ¸å›´ç­‰éœ€è¦å•†å“ä¿¡æ¯çš„é—®é¢˜ï¼Œè¯·åªè¿”å›{{product_inquiry å•†å“åºå·}}ï¼Œæå–ä¸åˆ°çš„ä¿¡æ¯è¯·ç½®ä¸º0;"
                              f"å¦åˆ™ï¼Œè¯·è¿”å›other")

            print(f"User: {_parse_text(_query)}")
            _chatbot.append((_parse_text(_query), ""))
            full_response = ""

            for response in model.chat_stream(tokenizer, modified_query, history=_task_history, generation_config=config):
                # _chatbot[-1] = (_parse_text(_query), _parse_text(response))

                # yield _chatbot
                full_response = _parse_text(response)

            print(f"History: {_task_history}")
            # _task_history.append((_query, full_response))
            print(f"Qwen-Chat: {_parse_text(full_response)}")

            question_type, details = parse_user_input(full_response)

            if question_type == "size_inquiry":
                print("é—®å°ºç ")
                product_info = products
                # æ„å»ºæ–°çš„æŸ¥è¯¢ï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„å•†å“ä¿¡æ¯
                # if session.height != 0 or session.weight != 0:
                #     modified_query = f"ç”¨æˆ·é—®é¢˜æ˜¯{_query}. å°ºç è¡¨æ˜¯: {cm_info}ï¼Œè¯·å›ç­”ç”¨æˆ·é—®é¢˜"
                # else:
                modified_query = f"ç”¨æˆ·é—®é¢˜æ˜¯{_query}. å°ºç è¡¨æ˜¯: {cm_info}ï¼Œè¯·åˆ¤æ–­ä¿¡æ¯æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœä¸è¶³è¯·è¿”å›â€œå¾ˆæŠ±æ­‰ï¼Œè¯·æä¾›ç›¸å…³ä¿¡æ¯â€ï¼Œå¦‚æœå……è¶³ï¼Œå¯ä»¥å›ç­”ç”¨æˆ·é—®é¢˜"
            elif question_type == "product_inquiry":
                # æ£€ç´¢å•†å“ä¿¡æ¯æˆ–å°ºç æ¨è
                print("é—®å•†å“")
                product_info = products
                # æ„å»ºæ–°çš„æŸ¥è¯¢ï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„å•†å“ä¿¡æ¯
                modified_query = f"ç”¨æˆ·é—®é¢˜æ˜¯{_query}. å•†å“ä¿¡æ¯: {product_info}ï¼Œè¯·å›ç­”ç”¨æˆ·é—®é¢˜"
            else:
                print("å…¶ä»–")
                # å¦‚æœæŸ¥è¯¢ä¸éœ€è¦æ£€ç´¢æˆ–æ— æ³•è§£æï¼Œåˆ™ä¸ä¿®æ”¹
                modified_query = _query
            # è°ƒç”¨åŸå§‹çš„predictå‡½æ•°å¤„ç†ä¿®æ”¹åçš„æŸ¥è¯¢
            print(f"User: {_parse_text(_query)}")
            _chatbot.append((_parse_text(_query), ""))
            full_response = ""

            for response in model.chat_stream(tokenizer, modified_query, history=_task_history, generation_config=config):
                _chatbot[-1] = (_parse_text(modified_query), _parse_text(response))

                yield _chatbot
                full_response = _parse_text(response)

            print(f"History: {_task_history}")
            _task_history.append((_query, full_response))
            print(f"Qwen-Chat: {_parse_text(full_response)}")




    def handle_user_input(session, user_input):
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç­‰å¾…ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯
        if session.is_awaiting_details():
            # å‡è®¾æ‰€æœ‰å¾…è¡¥å……ä¿¡æ¯éƒ½é€šè¿‡è¿™ä¸ªè¾“å…¥æä¾›
            session.update_last_question(user_input)
            question_type, details = session.get_last_question()
            response = generate_response(question_type, {"additional_info": user_input})
            return response, session

        # è§£æç”¨æˆ·çš„æ–°è¾“å…¥
        question_type, details = parse_user_input(user_input)
        session.add_question(question_type, details)

        if question_type == "unknown":
            return "æˆ‘ä»¬æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·å°è¯•æä¾›æ›´å¤šçš„ä¿¡æ¯ã€‚", session
        elif session.is_awaiting_details():
            return request_additional_info(question_type), session
        else:
            response = generate_response(question_type, details)
            return response, session

    def regenerate(_chatbot, _task_history):
        if not _task_history:
            yield _chatbot
            return
        item = _task_history.pop(-1)
        _chatbot.pop(-1)
        yield from predict(item[0], _chatbot, _task_history)

    def reset_user_input():
        return gr.update(value="")

    def reset_state(_chatbot, _task_history):
        _task_history.clear()
        _chatbot.clear()
        _gc()
        return _chatbot

    with gr.Blocks() as demo:
        gr.Markdown("""\
<p align="center"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.GdrPUkJGIYxbQ_jFgqO-dAHaE7?rs=1&pid=ImgDetMain" style="height: 80px"/><p>""")
        gr.Markdown("""<center><font size=8>å…±äº«å“è´¨åŒ—ç¾ç”Ÿæ´»é¦†</center>""")
        gr.Markdown(
            """\
<center><font size=3>æœ¬WebUIå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚</center>""")
        #         gr.Markdown("""\
        # <center><font size=4>
        # Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– </a> |
        # <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ
        # Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– </a> |
        # <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp ï½œ
        # Qwen-14B <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤– </a> |
        # <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a>&nbsp ï½œ
        # Qwen-14B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤– </a> |
        # <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a>&nbsp ï½œ
        # &nbsp<a href="https://github.com/QwenLM/Qwen">Github</a></center>""")

        chatbot = gr.Chatbot(label='èŠå¤©æ¡†', elem_classes="control-height")
        query = gr.Textbox(lines=2, label='è¾“å…¥')
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            submit_btn = gr.Button("ğŸš€ Submit (å‘é€)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

        # submit_btn.click(predict, [query, chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(predict1, [query, chatbot, task_history], [chatbot], show_progress=True)

        submit_btn.click(reset_user_input, [], [query])
        empty_btn.click(reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)

    #         gr.Markdown("""\
    # <font size=2>Note: This demo is governed by the original license of Qwen. \
    # We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \
    # including hate speech, violence, pornography, deception, etc. \
    # (æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwençš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\
    # åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)""")

    demo.queue().launch(
        share=args.share,
        inbrowser=args.inbrowser,
        server_port=args.server_port,
        server_name=args.server_name,
    )


def main():
    args = _get_args()

    model, tokenizer, config = _load_model_tokenizer(args)

    _launch_demo(args, model, tokenizer, config)


if __name__ == '__main__':
    main()
